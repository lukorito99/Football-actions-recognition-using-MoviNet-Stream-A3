{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdbcc5a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-28T11:04:51.460920Z",
     "iopub.status.busy": "2024-11-28T11:04:51.460511Z",
     "iopub.status.idle": "2024-11-28T11:04:56.430998Z",
     "shell.execute_reply": "2024-11-28T11:04:56.430200Z"
    },
    "papermill": {
     "duration": 4.979896,
     "end_time": "2024-11-28T11:04:56.432898",
     "exception": false,
     "start_time": "2024-11-28T11:04:51.453002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        pass\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c0713d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:04:56.444197Z",
     "iopub.status.busy": "2024-11-28T11:04:56.443801Z",
     "iopub.status.idle": "2024-11-28T11:07:20.474542Z",
     "shell.execute_reply": "2024-11-28T11:07:20.473376Z"
    },
    "papermill": {
     "duration": 144.038645,
     "end_time": "2024-11-28T11:07:20.476819",
     "exception": false,
     "start_time": "2024-11-28T11:04:56.438174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCollecting nvidia-cudnn-cu11\r\n",
      "  Downloading nvidia_cudnn_cu11-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu11 (from nvidia-cudnn-cu11)\r\n",
      "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Downloading nvidia_cudnn_cu11-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (566.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.7/566.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11\r\n",
      "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cudnn-cu11-9.5.1.17\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q \"tf-models-official\"\n",
    "!pip install -U nvidia-cudnn-cu11\n",
    "\n",
    "# Install the mediapy package for visualizing images/videos.\n",
    "# See https://github.com/google/mediapy\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy remotezip\n",
    "!pip install -U -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db1e634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:07:20.512605Z",
     "iopub.status.busy": "2024-11-28T11:07:20.511706Z",
     "iopub.status.idle": "2024-11-28T11:07:57.069247Z",
     "shell.execute_reply": "2024-11-28T11:07:57.067989Z"
    },
    "papermill": {
     "duration": 36.577554,
     "end_time": "2024-11-28T11:07:57.071417",
     "exception": false,
     "start_time": "2024-11-28T11:07:20.493863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tf-models-official  --quiet\n",
    "!pip install tensorflow-docs --quiet\n",
    "!pip install -U kaleido --quiet\n",
    "!pip install tf-keras-vis --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a5f24f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:07:57.106720Z",
     "iopub.status.busy": "2024-11-28T11:07:57.106412Z",
     "iopub.status.idle": "2024-11-28T11:08:03.055187Z",
     "shell.execute_reply": "2024-11-28T11:08:03.054492Z"
    },
    "papermill": {
     "duration": 5.968446,
     "end_time": "2024-11-28T11:08:03.057162",
     "exception": false,
     "start_time": "2024-11-28T11:07:57.088716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 11:07:59.013487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732792079.035998      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732792079.043029      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0806ef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:03.091751Z",
     "iopub.status.busy": "2024-11-28T11:08:03.091236Z",
     "iopub.status.idle": "2024-11-28T11:08:03.095426Z",
     "shell.execute_reply": "2024-11-28T11:08:03.094665Z"
    },
    "papermill": {
     "duration": 0.022819,
     "end_time": "2024-11-28T11:08:03.096932",
     "exception": false,
     "start_time": "2024-11-28T11:08:03.074113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2251f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:03.130695Z",
     "iopub.status.busy": "2024-11-28T11:08:03.130419Z",
     "iopub.status.idle": "2024-11-28T11:08:04.027358Z",
     "shell.execute_reply": "2024-11-28T11:08:04.026661Z"
    },
    "papermill": {
     "duration": 0.915912,
     "end_time": "2024-11-28T11:08:04.029266",
     "exception": false,
     "start_time": "2024-11-28T11:08:03.113354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model\n",
    "from official.projects.movinet.tools import export_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc83453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:04.064642Z",
     "iopub.status.busy": "2024-11-28T11:08:04.064360Z",
     "iopub.status.idle": "2024-11-28T11:08:31.264585Z",
     "shell.execute_reply": "2024-11-28T11:08:31.263709Z"
    },
    "papermill": {
     "duration": 27.22009,
     "end_time": "2024-11-28T11:08:31.266552",
     "exception": false,
     "start_time": "2024-11-28T11:08:04.046462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 8\n",
      "Num videos for class shortpass: 354\n",
      "Num videos for class longpass: 147\n",
      "Num videos for class throw: 367\n",
      "Num videos for class goalkick: 425\n",
      "Num videos for class penalty: 322\n",
      "Num videos for class corner: 411\n",
      "Num videos for class freekick: 374\n",
      "Num videos for class ontarget: 340\n",
      "\n",
      "Average frames and length per class:\n",
      "shortpass:\n",
      "  Avg frames: 75.86\n",
      "  Avg length: 2.99 seconds\n",
      "longpass:\n",
      "  Avg frames: 101.80\n",
      "  Avg length: 4.07 seconds\n",
      "throw:\n",
      "  Avg frames: 62.64\n",
      "  Avg length: 2.46 seconds\n",
      "goalkick:\n",
      "  Avg frames: 73.02\n",
      "  Avg length: 2.88 seconds\n",
      "penalty:\n",
      "  Avg frames: 71.91\n",
      "  Avg length: 2.86 seconds\n",
      "corner:\n",
      "  Avg frames: 75.38\n",
      "  Avg length: 2.89 seconds\n",
      "freekick:\n",
      "  Avg frames: 74.21\n",
      "  Avg length: 2.88 seconds\n",
      "ontarget:\n",
      "  Avg frames: 69.32\n",
      "  Avg length: 2.48 seconds\n"
     ]
    }
   ],
   "source": [
    "def classes_available(path):\n",
    "    return [c for c in os.listdir(path)]\n",
    "\n",
    "def files_per_class(path):\n",
    "    class_instances = dict()\n",
    "    for c in classes_available(path):\n",
    "        class_path = os.path.join(path,c)\n",
    "        files = list()\n",
    "        for file in os.listdir(class_path):\n",
    "            files.append(os.path.join(class_path,file))\n",
    "        class_instances[c.split('-')[0]] = files\n",
    "\n",
    "    return class_instances\n",
    "\n",
    "t = files_per_class('/kaggle/input/footballactions/FootballActions')\n",
    "\n",
    "print('Num classes:', len(list(t.keys())))\n",
    "for i in t.keys():\n",
    "    print(f'Num videos for class {i}:' ,len(t[i]))\n",
    "\n",
    "\n",
    "def average_frames_and_length_per_class(path):\n",
    "    class_instances = files_per_class(path)\n",
    "    avg_frames_per_class = {}\n",
    "    avg_length_per_class = {}\n",
    "\n",
    "    for class_name, files in class_instances.items():\n",
    "        total_frames = 0\n",
    "        total_length = 0\n",
    "        for file in files:\n",
    "            cap = cv2.VideoCapture(file)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            duration = frame_count / fps if fps > 0 else 0\n",
    "\n",
    "            total_frames += frame_count\n",
    "            total_length += duration\n",
    "            cap.release()\n",
    "\n",
    "        num_files = len(files)\n",
    "        avg_frames = total_frames / num_files if num_files else 0\n",
    "        avg_length = total_length / num_files if num_files else 0\n",
    "\n",
    "        avg_frames_per_class[class_name] = avg_frames\n",
    "        avg_length_per_class[class_name] = avg_length\n",
    "\n",
    "    return avg_frames_per_class, avg_length_per_class\n",
    "\n",
    "\n",
    "avg_frames_per_class, avg_length_per_class = average_frames_and_length_per_class('/kaggle/input/footballactions/FootballActions')\n",
    "\n",
    "print('\\nAverage frames and length per class:')\n",
    "for class_name in avg_frames_per_class.keys():\n",
    "    print(f'{class_name}:')\n",
    "    print(f'  Avg frames: {avg_frames_per_class[class_name]:.2f}')\n",
    "    print(f'  Avg length: {avg_length_per_class[class_name]:.2f} seconds')\n",
    "\n",
    "def train_test_split(dataset_dict, test_split=0.2, seed=42):\n",
    "    random.seed(seed)\n",
    "    train_dict = dict()\n",
    "    test_dict = dict()\n",
    "\n",
    "    for class_name, files in dataset_dict.items():\n",
    "        random.shuffle(files)\n",
    "        split_index = int(len(files) * (1 - test_split))\n",
    "        train_files = files[:split_index]\n",
    "        test_files = files[split_index:]\n",
    "        train_dict[class_name] = train_files\n",
    "        test_dict[class_name] = test_files\n",
    "\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e80582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:31.302273Z",
     "iopub.status.busy": "2024-11-28T11:08:31.301445Z",
     "iopub.status.idle": "2024-11-28T11:08:31.394154Z",
     "shell.execute_reply": "2024-11-28T11:08:31.393541Z"
    },
    "papermill": {
     "duration": 0.112365,
     "end_time": "2024-11-28T11:08:31.395993",
     "exception": false,
     "start_time": "2024-11-28T11:08:31.283628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def compute_inverse_class_weights(dataset_dict):\n",
    "    \"\"\"\n",
    "    Compute class weights using inverse frequency.\n",
    "    Higher weight for classes with fewer samples.\n",
    "    \"\"\"\n",
    "    # Get sample counts for each class\n",
    "    class_counts = {class_name: len(files) for class_name, files in dataset_dict.items()}\n",
    "    total_samples = sum(class_counts.values())\n",
    "    \n",
    "    # Compute weights inversely proportional to class frequencies\n",
    "    class_weights = {}\n",
    "    for class_name, count in class_counts.items():\n",
    "        class_weights[class_name] = total_samples / (len(class_counts) * count)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def compute_balanced_class_weights(dataset_dict):\n",
    "    \"\"\"\n",
    "    Compute class weights using sklearn's balanced weighting.\n",
    "    This generally provides better balance than simple inverse frequency.\n",
    "    \"\"\"\n",
    "    # Prepare data for sklearn's compute_class_weight\n",
    "    classes = list(dataset_dict.keys())\n",
    "    y = []\n",
    "    for class_name, files in dataset_dict.items():\n",
    "        y.extend([class_name] * len(files))\n",
    "    \n",
    "    # Compute balanced weights\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y),\n",
    "        y=y\n",
    "    )\n",
    "    \n",
    "    return dict(zip(classes, weights))\n",
    "\n",
    "def compute_effective_samples_weights(dataset_dict, beta=0.999):\n",
    "    \"\"\"\n",
    "    Compute class weights using effective number of samples.\n",
    "    This method helps prevent the model from being too aggressive \n",
    "    in favoring minority classes.\n",
    "    \n",
    "    Reference: \"Class-Balanced Loss Based on Effective Number of Samples\"\n",
    "    \"\"\"\n",
    "    # Get sample counts for each class\n",
    "    class_counts = {class_name: len(files) for class_name, files in dataset_dict.items()}\n",
    "    \n",
    "    # Compute effective number of samples for each class\n",
    "    class_weights = {}\n",
    "    for class_name, n in class_counts.items():\n",
    "        # Calculate effective number using the paper's formula\n",
    "        effective_num = (1.0 - beta ** n) / (1.0 - beta)\n",
    "        class_weights[class_name] = (1 - beta) / (1 - beta ** n)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(class_weights.values())\n",
    "    for class_name in class_weights:\n",
    "        class_weights[class_name] *= len(class_weights) / total_weight\n",
    "    \n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90228be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:31.431000Z",
     "iopub.status.busy": "2024-11-28T11:08:31.430657Z",
     "iopub.status.idle": "2024-11-28T11:08:31.464842Z",
     "shell.execute_reply": "2024-11-28T11:08:31.464201Z"
    },
    "papermill": {
     "duration": 0.05321,
     "end_time": "2024-11-28T11:08:31.466414",
     "exception": false,
     "start_time": "2024-11-28T11:08:31.413204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute weights using different methods\n",
    "inverse_weights = compute_inverse_class_weights(t)\n",
    "balanced_weights = compute_balanced_class_weights(t)\n",
    "effective_weights = compute_effective_samples_weights(t)\n",
    "\n",
    "# Create class index mapping\n",
    "class_indices = {name: idx for idx, name in enumerate(t.keys())}\n",
    "\n",
    "# Convert to index-based weights for all methods\n",
    "index_inverse_weights = {class_indices[k]: v for k, v in inverse_weights.items()}\n",
    "index_balanced_weights = {class_indices[k]: v for k, v in balanced_weights.items()}\n",
    "index_effective_weights = {class_indices[k]: v for k, v in effective_weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a38c6b65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:31.500993Z",
     "iopub.status.busy": "2024-11-28T11:08:31.500713Z",
     "iopub.status.idle": "2024-11-28T11:08:31.506771Z",
     "shell.execute_reply": "2024-11-28T11:08:31.505912Z"
    },
    "papermill": {
     "duration": 0.025203,
     "end_time": "2024-11-28T11:08:31.508502",
     "exception": false,
     "start_time": "2024-11-28T11:08:31.483299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "shortpass: 354 samples\n",
      "longpass: 147 samples\n",
      "throw: 367 samples\n",
      "goalkick: 425 samples\n",
      "penalty: 322 samples\n",
      "corner: 411 samples\n",
      "freekick: 374 samples\n",
      "ontarget: 340 samples\n",
      "\n",
      "Inverse frequency weights:\n",
      "Class 0: 0.9675\n",
      "Class 1: 2.3299\n",
      "Class 2: 0.9332\n",
      "Class 3: 0.8059\n",
      "Class 4: 1.0637\n",
      "Class 5: 0.8333\n",
      "Class 6: 0.9158\n",
      "Class 7: 1.0074\n",
      "\n",
      "Balanced weights (sklearn):\n",
      "Class 0: 0.8333\n",
      "Class 1: 0.9158\n",
      "Class 2: 0.8059\n",
      "Class 3: 2.3299\n",
      "Class 4: 1.0074\n",
      "Class 5: 1.0637\n",
      "Class 6: 0.9675\n",
      "Class 7: 0.9332\n",
      "\n",
      "Effective samples weights:\n",
      "Class 0: 0.8919\n",
      "Class 1: 1.9450\n",
      "Class 2: 0.8656\n",
      "Class 3: 0.7680\n",
      "Class 4: 0.9658\n",
      "Class 5: 0.7890\n",
      "Class 6: 0.8522\n",
      "Class 7: 0.9225\n"
     ]
    }
   ],
   "source": [
    "# Print results for comparison\n",
    "print(\"Class distribution:\")\n",
    "for class_name, files in t.items():\n",
    "    print(f\"{class_name}: {len(files)} samples\")\n",
    "\n",
    "print(\"\\nInverse frequency weights:\")\n",
    "for idx, weight in sorted(index_inverse_weights.items()):\n",
    "    print(f\"Class {idx}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nBalanced weights (sklearn):\")\n",
    "for idx, weight in sorted(index_balanced_weights.items()):\n",
    "    print(f\"Class {idx}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nEffective samples weights:\")\n",
    "for idx, weight in sorted(index_effective_weights.items()):\n",
    "    print(f\"Class {idx}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d53b65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:31.542851Z",
     "iopub.status.busy": "2024-11-28T11:08:31.542592Z",
     "iopub.status.idle": "2024-11-28T11:08:31.547545Z",
     "shell.execute_reply": "2024-11-28T11:08:31.546897Z"
    },
    "papermill": {
     "duration": 0.023904,
     "end_time": "2024-11-28T11:08:31.549107",
     "exception": false,
     "start_time": "2024-11-28T11:08:31.525203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_balanced_class_weights(train_dict):\n",
    "    \"\"\"\n",
    "    Compute balanced class weights using sklearn's implementation.\n",
    "    \n",
    "    Args:\n",
    "        train_dict: Dictionary mapping class names to lists of training samples\n",
    "    Returns:\n",
    "        Dictionary mapping class indices to their balanced weights\n",
    "    \"\"\"\n",
    "    # Create labels array for compute_class_weight\n",
    "    labels = []\n",
    "    label_to_index = {}\n",
    "    for idx, class_name in enumerate(train_dict.keys()):\n",
    "        label_to_index[class_name] = idx\n",
    "        labels.extend([idx] * len(train_dict[class_name]))\n",
    "    \n",
    "    # Compute balanced weights\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=labels\n",
    "    )\n",
    "    \n",
    "    # Create dictionary mapping class indices to weights\n",
    "    class_weights = dict(zip(range(len(weights)), weights))\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85ca910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:31.583293Z",
     "iopub.status.busy": "2024-11-28T11:08:31.583037Z",
     "iopub.status.idle": "2024-11-28T11:08:31.591977Z",
     "shell.execute_reply": "2024-11-28T11:08:31.591055Z"
    },
    "papermill": {
     "duration": 0.02817,
     "end_time": "2024-11-28T11:08:31.593766",
     "exception": false,
     "start_time": "2024-11-28T11:08:31.565596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using balanced class weights:\n",
      "Class 0: 0.9675\n",
      "Class 1: 2.3299\n",
      "Class 2: 0.9332\n",
      "Class 3: 0.8059\n",
      "Class 4: 1.0637\n",
      "Class 5: 0.8333\n",
      "Class 6: 0.9158\n",
      "Class 7: 1.0074\n"
     ]
    }
   ],
   "source": [
    "class_weights = get_balanced_class_weights(t)\n",
    "print(\"\\nUsing balanced class weights:\")\n",
    "for idx, weight in sorted(class_weights.items()):\n",
    "    print(f\"Class {idx}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541b1765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:31.629005Z",
     "iopub.status.busy": "2024-11-28T11:08:31.628725Z",
     "iopub.status.idle": "2024-11-28T11:08:32.933792Z",
     "shell.execute_reply": "2024-11-28T11:08:32.933130Z"
    },
    "papermill": {
     "duration": 1.324748,
     "end_time": "2024-11-28T11:08:32.935739",
     "exception": false,
     "start_time": "2024-11-28T11:08:31.610991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f91dbb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:32.972164Z",
     "iopub.status.busy": "2024-11-28T11:08:32.971569Z",
     "iopub.status.idle": "2024-11-28T11:08:32.982721Z",
     "shell.execute_reply": "2024-11-28T11:08:32.981946Z"
    },
    "papermill": {
     "duration": 0.030839,
     "end_time": "2024-11-28T11:08:32.984350",
     "exception": false,
     "start_time": "2024-11-28T11:08:32.953511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def per_model(t, train_dict, test_dict, model_class, split):\n",
    "    split_percentage = 100 * split\n",
    "    classes = list(t.keys())\n",
    "    train_counts = [len(train_dict[c]) for c in classes]\n",
    "    test_counts = [len(test_dict[c]) for c in classes]\n",
    "    \n",
    "    # Sort classes by total count\n",
    "    sorted_data = sorted(zip(classes, train_counts, test_counts), key=lambda x: x[1] + x[2], reverse=True)\n",
    "    sorted_classes, sorted_train_counts, sorted_test_counts = zip(*sorted_data)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add bars for train and test counts\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sorted_classes,\n",
    "        y=sorted_train_counts,\n",
    "        name='Train',\n",
    "        marker_color='#9370DB',  # Medium purple color\n",
    "        text=[f'{count:,} ({count/(count+test_count):.1%})' for count, test_count in zip(sorted_train_counts, sorted_test_counts)],\n",
    "        textposition='auto',\n",
    "        hoverinfo='text+name'\n",
    "    ))\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sorted_classes,\n",
    "        y=sorted_test_counts,\n",
    "        name='Test',\n",
    "        marker_color='#D3D3D3',  # Light gray color\n",
    "        text=[f'{count:,} ({count/(count+train_count):.1%})' for count, train_count in zip(sorted_test_counts, sorted_train_counts)],\n",
    "        textposition='auto',\n",
    "        hoverinfo='text+name'\n",
    "    ))\n",
    "\n",
    "    # Calculate overall statistics\n",
    "    total_train = sum(train_counts)\n",
    "    total_test = sum(test_counts)\n",
    "    total_overall = total_train + total_test\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f'Training-Test Split for {model_class}',\n",
    "            'font': {'family': 'Segoe Print', 'size': 24},\n",
    "            'y': 0.95,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        xaxis_title={'text': 'Class', 'font': {'family': 'Segoe Print', 'size': 18}},\n",
    "        yaxis_title={'text': 'Number of Videos', 'font': {'family': 'Segoe Print', 'size': 18}},\n",
    "        annotations=[\n",
    "            dict(\n",
    "                x=0.5, y=1.05,\n",
    "                showarrow=False,\n",
    "                text=f'Total Videos: {total_overall:,} | Train: {total_train:,} ({total_train/total_overall:.1%}) | Test: {total_test:,} ({total_test/total_overall:.1%})',\n",
    "                xref='paper', yref='paper',\n",
    "                font=dict(family='Segoe Print', size=16, color='darkred')\n",
    "            )\n",
    "        ],\n",
    "        hovermode='closest',\n",
    "        margin=dict(t=150),  # Increase top margin to accommodate the title and annotation\n",
    "        font=dict(family='Segoe Print'),  # Set global font\n",
    "        plot_bgcolor='#F8F8F8',  # Very light grey background\n",
    "        paper_bgcolor='#F8F8F8'  # Match the plot background color\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(tickangle=45, tickfont=dict(family='Segoe Print', size=12))\n",
    "    fig.update_yaxes(gridcolor='white', griddash='dash')  # Changed grid color to white for better contrast\n",
    "\n",
    "    # Add secondary y-axis for percentages\n",
    "    fig.update_layout(\n",
    "        yaxis2=dict(\n",
    "            title='Percentage',\n",
    "            titlefont=dict(family='Segoe Print', size=18),\n",
    "            overlaying='y',\n",
    "            side='right',\n",
    "            showgrid=False,\n",
    "            range=[0, 100],\n",
    "            ticksuffix='%'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save the figure\n",
    "    fig.write_image(f'detailed_train_test_split_distribution_for_{model_class}_at_training_split_{100-split_percentage:.0f}_{split_percentage:.0f}.png', scale=3, width=1200, height=800)\n",
    "    print(f\"Train-Test Split Distribution plot saved for {model_class} with {split_percentage:.1f}% split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "413147bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.019009Z",
     "iopub.status.busy": "2024-11-28T11:08:33.018725Z",
     "iopub.status.idle": "2024-11-28T11:08:33.022925Z",
     "shell.execute_reply": "2024-11-28T11:08:33.022166Z"
    },
    "papermill": {
     "duration": 0.023255,
     "end_time": "2024-11-28T11:08:33.024448",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.001193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "    Formats video frames to specified size.\n",
    "    \"\"\"\n",
    "    frame = cv2.resize(frame, output_size)\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "def create_blank_frames(n_frames, output_size):\n",
    "    \"\"\"\n",
    "    Creates blank frames for error cases.\n",
    "    \"\"\"\n",
    "    return np.zeros((n_frames, output_size[0], output_size[1], 3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93676825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.061134Z",
     "iopub.status.busy": "2024-11-28T11:08:33.060862Z",
     "iopub.status.idle": "2024-11-28T11:08:33.069254Z",
     "shell.execute_reply": "2024-11-28T11:08:33.068609Z"
    },
    "papermill": {
     "duration": 0.028152,
     "end_time": "2024-11-28T11:08:33.070701",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.042549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def frames_from_video_file_optimized(video_path, n_frames=8, output_size=(172,172)):\n",
    "    \"\"\"\n",
    "    Statistics-aware frame extraction optimized for memory constraints.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        \n",
    "        n_frames: Number of frames to extract\n",
    "        output_size: Output frame dimensions\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of frames: shape (n_frames, height, width, channels)\n",
    "    \"\"\"\n",
    "    src = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(src.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = src.get(cv2.CAP_PROP_FPS)\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # Ensure we have at least n_frames frames to sample\n",
    "    if total_frames < n_frames:\n",
    "        src.release()\n",
    "        return create_blank_frames(n_frames, output_size)\n",
    "    \n",
    "    frame_step = max(total_frames // (n_frames + 1), 1)\n",
    "    \n",
    "    # Determine sampling strategy based on video duration\n",
    "    if duration > 5.0:  # Long videos\n",
    "        # Sample evenly with focus on middle section\n",
    "        mid_point = total_frames // 2\n",
    "        quarter = total_frames // 4\n",
    "        frame_positions = [\n",
    "            quarter,                        # First quarter\n",
    "            mid_point - frame_step,         # Pre-middle\n",
    "            mid_point,                      # Middle\n",
    "            mid_point + frame_step,         # Post-middle\n",
    "            3 * quarter,                    # Third quarter\n",
    "            total_frames - 2*frame_step,    # Near end\n",
    "            total_frames - frame_step,      # Pre-end\n",
    "            total_frames - 1                # End\n",
    "        ]\n",
    "    elif duration > 3.0:  # Medium length videos\n",
    "        # Balanced sampling\n",
    "        frame_positions = np.linspace(0, total_frames-1, n_frames, dtype=int)\n",
    "    else:  # Short videos\n",
    "        # Dense sampling around the middle\n",
    "        mid_point = total_frames // 2\n",
    "        offset = frame_step // 2\n",
    "        frame_positions = [\n",
    "            0,                              # Start\n",
    "            mid_point - 2*offset,           # Pre-action\n",
    "            mid_point - offset,             # Action build-up\n",
    "            mid_point,                      # Peak action\n",
    "            mid_point + offset,             # Post-peak\n",
    "            mid_point + 2*offset,           # Action follow-through\n",
    "            total_frames - frame_step,      # Pre-end\n",
    "            total_frames - 1                # End\n",
    "        ]\n",
    "    \n",
    "    result = []\n",
    "    for pos in frame_positions:\n",
    "        src.set(cv2.CAP_PROP_POS_FRAMES, min(pos, total_frames - 1))\n",
    "        ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = format_frames(frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            result.append(np.zeros((output_size[0], output_size[1], 3), dtype=np.float32))\n",
    "    \n",
    "    src.release()\n",
    "    result = np.array(result)[..., [2, 1, 0]]  # BGR to RGB\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251c9f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.105587Z",
     "iopub.status.busy": "2024-11-28T11:08:33.105343Z",
     "iopub.status.idle": "2024-11-28T11:08:33.111657Z",
     "shell.execute_reply": "2024-11-28T11:08:33.110852Z"
    },
    "papermill": {
     "duration": 0.025701,
     "end_time": "2024-11-28T11:08:33.113199",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.087498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class FrameGenerator:\n",
    "    def __init__(self, dataset_dict, num_frames, size=290, training=False):\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.num_frames = num_frames\n",
    "        self.size = size\n",
    "        self.training = training\n",
    "        self.class_names = sorted(dataset_dict.keys())\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "    \n",
    "    def __call__(self):\n",
    "        video_paths = []\n",
    "        classes = []\n",
    "        for class_name, files in self.dataset_dict.items():\n",
    "            video_paths.extend(files)\n",
    "            classes.extend([class_name] * len(files))\n",
    "        \n",
    "        pairs = list(zip(video_paths, classes))\n",
    "        if self.training:\n",
    "            random.shuffle(pairs)\n",
    "            \n",
    "        for video_path, class_name in pairs:\n",
    "            try:\n",
    "                video_frames = frames_from_video_file_optimized(video_path, \n",
    "                                                   self.num_frames, \n",
    "                                                   (self.size, self.size))\n",
    "                label = self.class_ids_for_name[class_name]\n",
    "                yield video_frames, label\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing video {video_path}: {str(e)}\")\n",
    "                yield create_blank_frames(self.num_frames, (self.size, self.size)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da90f393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.148216Z",
     "iopub.status.busy": "2024-11-28T11:08:33.147964Z",
     "iopub.status.idle": "2024-11-28T11:08:33.151536Z",
     "shell.execute_reply": "2024-11-28T11:08:33.150848Z"
    },
    "papermill": {
     "duration": 0.022998,
     "end_time": "2024-11-28T11:08:33.153174",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.130176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "#         'A0': {'batch_size': 10, 'num_frames': 8, 'resolution': 172,'split':0.30}\n",
    "        'A3': {'batch_size': 8, 'num_frames': 8, 'resolution': 256,'split':0.25}\n",
    "        # 'A2': {'batch_size': 10, 'num_frames': 8, 'resolution': 224,'split':0.30}\n",
    "       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ef8280a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.187929Z",
     "iopub.status.busy": "2024-11-28T11:08:33.187648Z",
     "iopub.status.idle": "2024-11-28T11:08:33.191754Z",
     "shell.execute_reply": "2024-11-28T11:08:33.191054Z"
    },
    "papermill": {
     "duration": 0.023303,
     "end_time": "2024-11-28T11:08:33.193312",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.170009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(generator,batch_size):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int16)\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a659f360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.228856Z",
     "iopub.status.busy": "2024-11-28T11:08:33.228609Z",
     "iopub.status.idle": "2024-11-28T11:08:33.232596Z",
     "shell.execute_reply": "2024-11-28T11:08:33.231974Z"
    },
    "papermill": {
     "duration": 0.023809,
     "end_time": "2024-11-28T11:08:33.234143",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.210334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
    "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "  model = movinet_model.MovinetClassifier(\n",
    "      backbone=backbone,\n",
    "      num_classes=num_classes)\n",
    "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98843c78",
   "metadata": {
    "papermill": {
     "duration": 0.016937,
     "end_time": "2024-11-28T11:08:33.268093",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.251156",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00df259f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.302829Z",
     "iopub.status.busy": "2024-11-28T11:08:33.302574Z",
     "iopub.status.idle": "2024-11-28T11:08:33.311313Z",
     "shell.execute_reply": "2024-11-28T11:08:33.310494Z"
    },
    "papermill": {
     "duration": 0.028087,
     "end_time": "2024-11-28T11:08:33.312907",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.284820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomLearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        tf.summary.scalar('learning_rate', data=lr, step=epoch)\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        logs['lr'] = lr\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = logs.get('lr', None)\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "class CustomReduceLROnPlateau(ReduceLROnPlateau):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = self.model.optimizer.lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class CustomEarlyStopping(EarlyStopping):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "class CustomLearningRateScheduler(LearningRateScheduler):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "class CustomTensorBoard(TensorBoard):\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe36d68c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.347580Z",
     "iopub.status.busy": "2024-11-28T11:08:33.347353Z",
     "iopub.status.idle": "2024-11-28T11:08:33.351045Z",
     "shell.execute_reply": "2024-11-28T11:08:33.350298Z"
    },
    "papermill": {
     "duration": 0.022779,
     "end_time": "2024-11-28T11:08:33.352613",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.329834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcacc67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.387784Z",
     "iopub.status.busy": "2024-11-28T11:08:33.387558Z",
     "iopub.status.idle": "2024-11-28T11:08:33.461347Z",
     "shell.execute_reply": "2024-11-28T11:08:33.460684Z"
    },
    "papermill": {
     "duration": 0.093758,
     "end_time": "2024-11-28T11:08:33.463311",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.369553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def get_actual_predicted_labels(dataset, model):\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    for images, labels in dataset:\n",
    "        actual.extend(labels.numpy())\n",
    "        preds = model.predict(images)\n",
    "        predicted.extend(tf.argmax(preds, axis=1).numpy())\n",
    "    return np.array(actual), np.array(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "603f0613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.500576Z",
     "iopub.status.busy": "2024-11-28T11:08:33.500326Z",
     "iopub.status.idle": "2024-11-28T11:08:33.513374Z",
     "shell.execute_reply": "2024-11-28T11:08:33.512634Z"
    },
    "papermill": {
     "duration": 0.032982,
     "end_time": "2024-11-28T11:08:33.514939",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.481957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_confusion_matrix(actual, predicted, labels, ds_type, model_class, accuracy, split, normalize=False):\n",
    "    split = split * 100\n",
    "    \n",
    "    def create_cm_plot(ax, cm, cmap, title):\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap,\n",
    "                    xticklabels=labels, yticklabels=labels,\n",
    "                    annot_kws={\"size\": 10, \"weight\": \"bold\"}, square=True, \n",
    "                    cbar_kws={'shrink': .8}, ax=ax, linewidths=0.5, linecolor='gray')\n",
    "        \n",
    "        ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Predicted Action', fontsize=14, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel('Actual Action', fontsize=14, fontweight='bold', labelpad=15)\n",
    "        \n",
    "        # Improve tick label visibility\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=10, fontweight='bold')\n",
    "        plt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Add value annotations\n",
    "        thresh = cm.max() / 2\n",
    "        for i, j in np.ndindex(cm.shape):\n",
    "            ax.text(j + 0.5, i + 0.5, f\"{cm[i, j]:.0f}\" if not normalize else f\"{cm[i, j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                    fontweight='bold', fontsize=10)\n",
    "\n",
    "        # Add borders to the heatmap\n",
    "        for _, spine in ax.spines.items():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(2)\n",
    "            spine.set_color('black')\n",
    "\n",
    "    try:\n",
    "        actual = np.ravel(actual)\n",
    "        predicted = np.ravel(predicted)\n",
    "        cm = confusion_matrix(actual, predicted)\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2f'\n",
    "        else:\n",
    "            fmt = 'd'\n",
    "        \n",
    "        fig, (ax_cm, ax_guide) = plt.subplots(1, 2, figsize=(24, 12), \n",
    "                                              gridspec_kw={'width_ratios': [1.5, 1]})\n",
    "        \n",
    "        # Custom colormap for better visibility\n",
    "        cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "        create_cm_plot(ax_cm, cm, cmap, 'Confusion Matrix')\n",
    "        \n",
    "        # Add interpretation guide\n",
    "        ax_guide.axis('off')\n",
    "        guide_text = \"\"\"\n",
    "        Interpretation Guide:\n",
    "        \n",
    "        1. Diagonal Elements: \n",
    "           Represent correct predictions. Higher values are better.\n",
    "        \n",
    "        2. Off-Diagonal Elements: \n",
    "           Show misclassifications. Lower values are better.\n",
    "        \n",
    "        3. Row Sums: \n",
    "           Total actual instances for each class.\n",
    "        \n",
    "        4. Column Sums: \n",
    "           Total predicted instances for each class.\n",
    "        \n",
    "        5. Color Intensity: \n",
    "           Darker blues indicate higher values.\n",
    "        \n",
    "        6. Normalized vs. Raw: \n",
    "           - Raw: Shows actual counts\n",
    "           - Normalized: Shows proportions (sum to 1 for each row)\n",
    "        \n",
    "        7. Common Patterns:\n",
    "           - Strong Diagonal: Good overall performance\n",
    "           - Weak Diagonal: Poor overall performance\n",
    "           - Dark Off-Diagonal: Common misclassification\n",
    "        \n",
    "        8. Class Imbalance: \n",
    "           Look for significantly different row sums.\n",
    "        \n",
    "        9. Precision vs. Recall:\n",
    "           - Precision: Look down columns (few false positives)\n",
    "           - Recall: Look across rows (few false negatives)\n",
    "        \n",
    "        10. Overall Accuracy:\n",
    "            Sum of diagonal elements divided by total predictions.\n",
    "        \"\"\"\n",
    "        ax_guide.text(0, 1, guide_text, va='top', ha='left', fontsize=12, \n",
    "                      bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.3),\n",
    "                      fontweight='medium')\n",
    "        \n",
    "        plt.suptitle(f'Confusion Matrix: MoViNet-{model_class}-Stream ({ds_type})\\n'\n",
    "                     f'Accuracy: {accuracy*100:.4f}%', fontsize=22, fontweight='bold', y=1.02)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fname = f'confusion_matrix_{model_class}_{ds_type}_accuracy_{accuracy*100:.4f}.png'\n",
    "        plt.savefig(fname, bbox_inches='tight', dpi=600)\n",
    "        plt.close()\n",
    "        print(f\"Enhanced confusion matrix saved as {fname}\")\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(actual, predicted, target_names=labels, output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_fname = f'classification_report_{model_class}_{ds_type}_at_training_split_{100-split}_{split}.csv'\n",
    "        report_df.to_csv(report_fname)\n",
    "        print(f\"Classification report saved as {report_fname}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting confusion matrix: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ae407fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.551289Z",
     "iopub.status.busy": "2024-11-28T11:08:33.550735Z",
     "iopub.status.idle": "2024-11-28T11:08:33.556936Z",
     "shell.execute_reply": "2024-11-28T11:08:33.556161Z"
    },
    "papermill": {
     "duration": 0.026544,
     "end_time": "2024-11-28T11:08:33.558620",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.532076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix2(actual, predicted, labels, ds_type,model_class,accuracy,split):\n",
    "  cm = tf.math.confusion_matrix(actual, predicted)\n",
    "  split = 100 * split\n",
    "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "  sns.set(rc={'figure.figsize':(6, 16)})\n",
    "  sns.set(font_scale=1.4)\n",
    "  plt.title(f'Confusion Matrix: MoViNet-{model_class}-Stream ({ds_type})\\n'\n",
    "                  f'Accuracy: {accuracy*100:.4f}',\n",
    "                  fontsize=16, pad=16)\n",
    "  ax.set_xlabel('Predicted Action')\n",
    "  ax.set_ylabel('Actual Action')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.yticks(rotation=0)\n",
    "  ax.xaxis.set_ticklabels(labels)\n",
    "  ax.yaxis.set_ticklabels(labels)\n",
    "  plt.tight_layout()\n",
    "  fname = f'confusion_matrix_alternative_presentation_{model_class}_{ds_type}_accuracy_{accuracy*100:.4f}%_at_training_split_{100-split}_{split}.png'\n",
    "  plt.savefig(fname, bbox_inches='tight', dpi=300)\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d342d615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.598136Z",
     "iopub.status.busy": "2024-11-28T11:08:33.597794Z",
     "iopub.status.idle": "2024-11-28T11:08:33.615850Z",
     "shell.execute_reply": "2024-11-28T11:08:33.614937Z"
    },
    "papermill": {
     "duration": 0.041077,
     "end_time": "2024-11-28T11:08:33.617774",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.576697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def plot_training_metrics(results, model_id, initial_lr, batch_size, optimizer, resolution, split, accuracy):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics (accuracy, loss, and learning rate) over epochs with improved visuals for print.\n",
    "    \n",
    "    Args:\n",
    "    results (keras.callbacks.History): History object returned by model.fit().\n",
    "    model_id (str): Identifier for the model.\n",
    "    initial_lr (float): Initial learning rate.\n",
    "    batch_size (int): Batch size used for training.\n",
    "    optimizer (str): Name of the optimizer used.\n",
    "    resolution (int): Input resolution for the model.\n",
    "    split (float): Train-validation split ratio.\n",
    "    accuracy (float): Test accuracy of the model.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    history = results.history  # Extract the history dictionary from the History object\n",
    "    split = 100 * split\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    gs = GridSpec(4, 2, figure=fig, height_ratios=[1, 1, 1, 0.5])\n",
    "    \n",
    "    # Colors\n",
    "    colors = {'train': '#C71585',  # Deep pink\n",
    "              'val': '#696969',    # Dim gray\n",
    "              'lr': '#FFA500'}     # Orange\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = ['accuracy', 'loss']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = fig.add_subplot(gs[i, :])\n",
    "        epochs = range(1, len(history[metric]) + 1)\n",
    "        \n",
    "        # Training data\n",
    "        ax.plot(epochs, history[metric], color=colors['train'], label=f'Training {metric.capitalize()}', linewidth=2, marker='o')\n",
    "        \n",
    "        # Validation data\n",
    "        ax.plot(epochs, history[f'val_{metric}'], color=colors['val'], label=f'Validation {metric.capitalize()}', linewidth=2, marker='o')\n",
    "        \n",
    "        # Add annotations for best/worst points\n",
    "        best_epoch = np.argmax(history[f'val_{metric}']) if metric == 'accuracy' else np.argmin(history[f'val_{metric}'])\n",
    "        best_value = history[f'val_{metric}'][best_epoch]\n",
    "        ax.annotate(f'Best: {best_value:.4f}', xy=(best_epoch+1, best_value), xytext=(5, 5), \n",
    "                    textcoords='offset points', ha='left', va='bottom',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "        \n",
    "        ax.set_title(f'{metric.capitalize()} Over Epochs', fontsize=16)\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Learning Rate Over Epochs\n",
    "    ax = fig.add_subplot(gs[2, :])\n",
    "    if 'lr' in history:\n",
    "        ax.plot(epochs, history['lr'], color=colors['lr'], linewidth=2, marker='o')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title('Learning Rate Over Epochs', fontsize=16)\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Annotate initial and final learning rates\n",
    "        ax.annotate(f'Initial LR: {history[\"lr\"][0]:.2e}', xy=(1, history[\"lr\"][0]), xytext=(5, 5), \n",
    "                    textcoords='offset points', ha='left', va='bottom',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "        ax.annotate(f'Final LR: {history[\"lr\"][-1]:.2e}', xy=(len(epochs), history[\"lr\"][-1]), xytext=(5, 5), \n",
    "                    textcoords='offset points', ha='right', va='top',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Learning Rate data not available', ha='center', va='center', fontsize=16)\n",
    "    \n",
    "    # Info text\n",
    "    info_text = [f\"Model: {model_id}\",\n",
    "                 f\"Initial Learning Rate: {initial_lr}\",\n",
    "                 f\"Final Learning Rate: {history['lr'][-1]:.2e}\" if 'lr' in history else \"Final Learning Rate: Not available\",\n",
    "                 f\"Batch Size: {batch_size}\",\n",
    "                 f\"Optimizer: {optimizer}\",\n",
    "                 f\"Resolution: {resolution}\",\n",
    "                 f\"Total Epochs: {len(history['accuracy'])}\",\n",
    "                 f\"Early Stopping: {'Yes' if len(history['accuracy']) < 30 else 'No'}\",\n",
    "                 f\"Train-Val Split: {100 - split:.2f}/{split:.2f}\"]\n",
    "\n",
    "    # Summary text\n",
    "    summary_text = [f\"Test Accuracy: {accuracy*100:.2f}%\",\n",
    "                    f\"Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}% (Epoch {np.argmax(history['val_accuracy'])+1})\",\n",
    "                    f\"Lowest Val Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\",\n",
    "                    f\"Overfitting: {'Not observed' if max(history['accuracy']) - max(history['val_accuracy']) < 0.05 else 'Observed'}\"]\n",
    "\n",
    "    # Add info and summary as text\n",
    "    ax_info = fig.add_subplot(gs[3, 0])\n",
    "    ax_info.axis('off')\n",
    "    ax_info.text(0, 1, '\\n'.join(info_text), verticalalignment='top', fontsize=12)\n",
    "    ax_info.set_title('Model Information', fontsize=16)\n",
    "\n",
    "    ax_summary = fig.add_subplot(gs[3, 1])\n",
    "    ax_summary.axis('off')\n",
    "    ax_summary.text(0, 1, '\\n'.join(summary_text), verticalalignment='top', fontsize=12)\n",
    "    ax_summary.set_title('Performance Summary', fontsize=16)\n",
    "\n",
    "    # Overall title\n",
    "    fig.suptitle(f\"Training Metrics for MoviNet {model_id} Stream at {100 - split}%-{split}% train test split\", fontsize=20, y=0.95)\n",
    "\n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"training_metrics_for_{model_id}_at_train-test_split_of_{split}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd9bd9b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.658712Z",
     "iopub.status.busy": "2024-11-28T11:08:33.658426Z",
     "iopub.status.idle": "2024-11-28T11:08:33.663677Z",
     "shell.execute_reply": "2024-11-28T11:08:33.662707Z"
    },
    "papermill": {
     "duration": 0.02786,
     "end_time": "2024-11-28T11:08:33.665461",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.637601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_representative_dataset(generator, batch_size):\n",
    "    \"\"\"\n",
    "    Create a representative dataset for TFLite integer quantization calibration.\n",
    "    \"\"\"\n",
    "    def representative_data_gen():\n",
    "        for _ in range(100):\n",
    "            data, _ = next(generator())\n",
    "            # Expand the batch dimension to match the expected input shape\n",
    "            yield [data[None, ...]]\n",
    "    return representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aae576f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.706056Z",
     "iopub.status.busy": "2024-11-28T11:08:33.705733Z",
     "iopub.status.idle": "2024-11-28T11:08:33.716368Z",
     "shell.execute_reply": "2024-11-28T11:08:33.715412Z"
    },
    "papermill": {
     "duration": 0.033474,
     "end_time": "2024-11-28T11:08:33.718191",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.684717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_model_variants(model, model_id, resolution, accuracy, split, train_dict, model_weights):\n",
    "    \"\"\"\n",
    "    Export the model in various formats for single-batch and multi-batch scenarios,\n",
    "    as well as multi-frame scenarios.\n",
    "    \n",
    "    Args:\n",
    "    model: The TensorFlow model to export\n",
    "    model_id (str): Identifier for the model\n",
    "    resolution (int): Input resolution for the model\n",
    "    accuracy (float): Accuracy of the model\n",
    "    \"\"\"\n",
    "    exported_models = {}\n",
    "    split = 100 * split\n",
    "\n",
    "    # Calculate initial model size and parameters\n",
    "    initial_model_size = get_model_size(model_weights)\n",
    "    initial_params = model.count_params()\n",
    "\n",
    "    # Determine initial numerical operations\n",
    "    initial_num_ops = get_numerical_operations(model)\n",
    "\n",
    "    for batch_size, batch_type in zip([1, 2], ['single', 'dual']):\n",
    "        for num_frames in [8]:\n",
    "            saved_model_dir = f'saved_model_{batch_type}_batch_{num_frames}_frames'\n",
    "            \n",
    "            input_shape = [batch_size, num_frames, resolution, resolution, 3]\n",
    "            \n",
    "            export_saved_model.export_saved_model(\n",
    "                model=model,\n",
    "                input_shape=input_shape,\n",
    "                export_path=saved_model_dir,\n",
    "                causal=True,\n",
    "                bundle_input_init_states_fn=False\n",
    "            )\n",
    "            \n",
    "            # FP16 conversion\n",
    "            converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            converter.target_spec.supported_types = [tf.float16]\n",
    "            tflite_model = converter.convert()\n",
    "            \n",
    "            fp16_filename = f'model_{model_id}_operations_using_fp16_with_{num_frames}_frames_at_{batch_type}_batch_from_{accuracy*100:.2f}%_model_at_training_split_{100-split}_{split}.tflite'\n",
    "            with open(fp16_filename, 'wb') as f:\n",
    "                f.write(tflite_model)\n",
    "                \n",
    "#             #INT8 conversion\n",
    "#             converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "#             converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#             converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "#             representative_dataset = create_representative_dataset(\n",
    "#                 FrameGenerator(train_dict, num_frames, resolution, training=False),\n",
    "#                 batch_size\n",
    "#             )\n",
    "#             converter.representative_dataset = representative_dataset\n",
    "\n",
    "#             tflite_model = converter.convert()\n",
    "            \n",
    "#             int8_filename = f'model_{model_id}_operations_using_int8_with_{num_frames}_frames_at_{batch_type}_batch_from_{accuracy*100:.2f}%_model_at_training_split_{100-split}_{split}.tflite'\n",
    "#             with open(int8_filename, 'wb') as f:\n",
    "#                 f.write(tflite_model)\n",
    "                \n",
    "                                                          \n",
    "            # Calculate exported model size\n",
    "            exported_model_size = os.path.getsize(fp16_filename)\n",
    "            \n",
    "            exported_models[f'fp16_{batch_type}batch_{num_frames}frames'] = {\n",
    "                'filename': fp16_filename,\n",
    "                'input_shape': input_shape,\n",
    "                'initial_model_size': initial_model_size,\n",
    "                'exported_model_size': exported_model_size,\n",
    "                'initial_num_ops': initial_num_ops,\n",
    "                'exported_num_ops': 'fp16',\n",
    "                'total_parameters': initial_params\n",
    "            }\n",
    "            \n",
    "#             exported_models[f'int8_{batch_type}batch_{num_frames}frames'] = {\n",
    "#                 'filename': int8_filename,\n",
    "#                 'input_shape': input_shape,\n",
    "#                 'initial_model_size': initial_model_size,\n",
    "#                 'exported_model_size': os.path.getsize(int8_filename),\n",
    "#                 'initial_num_ops': initial_num_ops,\n",
    "#                 'exported_num_ops': 'int8',\n",
    "#                 'total_parameters': initial_params\n",
    "#             }\n",
    "\n",
    "    # Save information to JSON file\n",
    "    json_filename = f'model_{model_id}_export_info.json'\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(exported_models, f, indent=4)\n",
    "\n",
    "    return exported_models\n",
    "\n",
    "def get_model_size(model_weights):\n",
    "    \n",
    "    size = os.path.getsize(model_weights)\n",
    "    return size\n",
    "\n",
    "def get_numerical_operations(model):\n",
    "    # Determine the numerical operations used in the model\n",
    "    if tf.keras.mixed_precision.global_policy().name == 'mixed_float16':\n",
    "        return 'mixed_float16'\n",
    "    else:\n",
    "        return 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdf4879c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.761460Z",
     "iopub.status.busy": "2024-11-28T11:08:33.760529Z",
     "iopub.status.idle": "2024-11-28T11:08:33.780666Z",
     "shell.execute_reply": "2024-11-28T11:08:33.779708Z"
    },
    "papermill": {
     "duration": 0.043878,
     "end_time": "2024-11-28T11:08:33.782536",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.738658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_tflite_evaluation_results(results, label_names, model_id, resolution, split):\n",
    "    plt.figure(figsize=(30, 40))\n",
    "    split = 100 * split\n",
    "    # Color palette\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 8))\n",
    "    \n",
    "    # 1. Accuracies bar plot\n",
    "    plt.subplot(4, 2, (1, 2))\n",
    "    model_types = list(results.keys())\n",
    "    accuracies = [res['accuracy'] for res in results.values()]\n",
    "    bars = plt.bar(model_types, accuracies, color=colors)\n",
    "    plt.title(f'TFLite Model Accuracies (MoViNet-{model_id.upper()}, Resolution: {resolution})', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 2. Input shapes and correct predictions\n",
    "    plt.subplot(4, 2, 3)\n",
    "    for i, (model_type, res) in enumerate(results.items()):\n",
    "        input_shape = 'x'.join(map(str, res['input_shape']))\n",
    "        correct = res['correct_predictions']\n",
    "        total = res['total_predictions']\n",
    "        plt.text(0, 1 - i / len(results), \n",
    "                 f\"{model_type} (Shape: {input_shape}):\\n{correct}/{total} correct\",\n",
    "                 fontsize=12, va='top')\n",
    "    plt.axis('off')\n",
    "    plt.title('Model Variants and Predictions', fontsize=16)\n",
    "    \n",
    "    # 3. Confusion matrix heatmap\n",
    "    plt.subplot(4, 2, (4, 5))\n",
    "    best_model = max(results, key=lambda x: results[x]['accuracy'])\n",
    "    confusion_mat = results[best_model]['confusion_matrix']\n",
    "    ax = plt.gca()\n",
    "    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='YlOrRd', xticklabels=label_names, yticklabels=label_names, cbar=True,ax=ax)\n",
    "    plt.title(f'Confusion Matrix for Best Model ({best_model})', fontsize=16)\n",
    "    plt.xlabel('Predicted Action', fontsize=14)\n",
    "    plt.ylabel('True Action', fontsize=14)\n",
    "    \n",
    "    # Add interpretation guide as a legend\n",
    "    guide_elements = [\n",
    "        plt.Rectangle((0,0),1,1,fc=\"white\", ec=\"gray\", lw=2),\n",
    "        plt.Rectangle((0,0),1,1,fc=\"lightgray\", ec=\"gray\", lw=2),\n",
    "        plt.Rectangle((0,0),1,1,fc=\"darkgray\", ec=\"gray\", lw=2),\n",
    "        Line2D([0], [0], color=\"black\", lw=2)\n",
    "    ]\n",
    "    guide_labels = [\n",
    "        \"Correct predictions (diagonal)\",\n",
    "        \"Minor misclassifications\",\n",
    "        \"Major misclassifications\",\n",
    "        \"Perfect classification line\"\n",
    "    ]\n",
    "    ax.legend(guide_elements, guide_labels, loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "              title=\"Interpretation Guide\", title_fontsize='large', fontsize='medium')\n",
    "    \n",
    "    # 4. Side-by-side comparison of TFLite accuracies\n",
    "    plt.subplot(4, 2, 6)\n",
    "    quantization_types = ['fp16']\n",
    "    batch_types = ['single', 'dual', 'triple', 'quad']\n",
    "    frame_counts = [4, 8, 12]\n",
    "    \n",
    "    x = np.arange(len(batch_types))\n",
    "    width = 0.2\n",
    "    \n",
    "   \n",
    "    for j, frame_count in enumerate(frame_counts):\n",
    "        try:\n",
    "            accuracies = [results[f'{quant_type}_{batch}batch_{frame_count}frames']['accuracy'] for batch in batch_types]\n",
    "            plt.bar(x + (i*3 + j) * width, accuracies, width, label=f'{quant_type.upper()} ({frame_count} frames)', color=colors[i*3 + j])\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing data for {e}, skipping plot.\")\n",
    "            continue\n",
    "    \n",
    "    plt.xlabel('Batch Type', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.title('TFLite Accuracies: Quantization vs Batch Size vs Frame Count', fontsize=16)\n",
    "    plt.xticks(x + width*3, batch_types, fontsize=12)\n",
    "    plt.legend(title='Quantization & Frames', title_fontsize=12, fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 5. Latency comparison (if available)\n",
    "    if 'latency' in next(iter(results.values())):\n",
    "        plt.subplot(4, 2, 7)\n",
    "        latencies = [res['latency'] for res in results.values()]\n",
    "        plt.bar(model_types, latencies, color=colors)\n",
    "        plt.title('Inference Latency Comparison', fontsize=16)\n",
    "        plt.xlabel('Model Type', fontsize=14)\n",
    "        plt.ylabel('Latency (ms)', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 6. Guide for interpreting the plots\n",
    "    plt.subplot(4, 2, 8)\n",
    "    plt.axis('off')\n",
    "    plt.text(0, 1, \"Guide for Interpreting the Plots:\", fontsize=16, fontweight='bold')\n",
    "    guide_text = \"\"\"\n",
    "    1. Accuracies Bar Plot: Shows the accuracy of each model variant. Higher bars indicate better performance.\n",
    "    \n",
    "    2. Model Variants and Predictions: Provides details on input shapes and correct predictions for each model.\n",
    "    \n",
    "    3. Confusion Matrix: Displays prediction errors for the best-performing model. Diagonal elements represent correct predictions.\n",
    "    \n",
    "    4. TFLite Accuracies Comparison: Compares accuracies across different quantization types, batch sizes, and frame counts.\n",
    "    \n",
    "    5. Latency Comparison (if available): Shows inference time for each model variant. Lower bars indicate faster inference.\n",
    "    \n",
    "    Key Observations:\n",
    "    - Look for the highest accuracy in the bar plots.\n",
    "    - Check if quantization (FP16) significantly affects accuracy.\n",
    "    - Observe how different batch sizes and frame counts impact performance.\n",
    "    - Consider the trade-off between accuracy and latency for deployment.\n",
    "    \"\"\"\n",
    "    plt.text(0, 0.9, guide_text, fontsize=12, va='top', ha='left', wrap=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'evaluated_tflite_accuracies_MoviNet_{model_id.upper()}_{resolution}_from_model_with_training_split_{100-split}_{split}.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41c566a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.823266Z",
     "iopub.status.busy": "2024-11-28T11:08:33.822933Z",
     "iopub.status.idle": "2024-11-28T11:08:33.836072Z",
     "shell.execute_reply": "2024-11-28T11:08:33.835101Z"
    },
    "papermill": {
     "duration": 0.034725,
     "end_time": "2024-11-28T11:08:33.837666",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.802941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_top_k(probs, k=5, label_map=None):\n",
    "    top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
    "    if label_map is not None:\n",
    "        top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
    "        top_labels = [label.decode('utf8') if isinstance(label, bytes) else label for label in top_labels.numpy()]\n",
    "    else:\n",
    "        top_labels = top_predictions.numpy().tolist()\n",
    "    top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
    "    return tuple(zip(top_labels, top_probs))\n",
    "\n",
    "def evaluate_exported_models(exported_models, test_ds, label_names, model_id, resolution, split):\n",
    "    results = {}\n",
    "    for model_type, model_data in exported_models.items():\n",
    "        model_path = model_data['filename']\n",
    "        input_shape = model_data['input_shape']\n",
    "        batch_size, num_frames = input_shape[0], input_shape[1]\n",
    "        print(f\"Evaluating {model_type} model from path {model_path}...\")\n",
    "        try:\n",
    "            interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "            interpreter.allocate_tensors()\n",
    "            runner = interpreter.get_signature_runner()\n",
    "            init_states = {\n",
    "                name: tf.zeros(x['shape'], dtype=x['dtype'])\n",
    "                for name, x in runner.get_input_details().items()\n",
    "                if name != 'image'\n",
    "            }\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            latencies = []\n",
    "            \n",
    "            for frames, labels in test_ds:\n",
    "                states = init_states.copy()\n",
    "                \n",
    "                # Handle different batch sizes\n",
    "                actual_batch_size = min(batch_size, frames.shape[0])\n",
    "                \n",
    "                # Prepare input based on model's expected shape\n",
    "                model_input = frames[:actual_batch_size, :num_frames]\n",
    "                \n",
    "                # Resize input if necessary to match expected shape\n",
    "                if model_input.shape[2:] != input_shape[2:4]:\n",
    "                    model_input = tf.image.resize(model_input, input_shape[2:4])\n",
    "                \n",
    "                # Ensure the input has the correct number of dimensions\n",
    "                if len(model_input.shape) == 4:  # If it's (batch, height, width, channels)\n",
    "                    model_input = tf.expand_dims(model_input, axis=1)  # Add time dimension\n",
    "                \n",
    "                # Reshape to match the expected input shape\n",
    "                model_input = tf.reshape(model_input, [actual_batch_size, num_frames] + list(input_shape[2:]))\n",
    "                \n",
    "                # Timing the inference\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Perform inference\n",
    "                outputs = runner(**states, image=model_input)\n",
    "                logits = outputs.pop('logits')\n",
    "                #states = outputs  # Update states with the remaining outputs\n",
    "                \n",
    "                latency = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "                latencies.append(latency)\n",
    "                \n",
    "                # Process predictions\n",
    "                probs = tf.nn.softmax(logits)\n",
    "                for i in range(actual_batch_size):\n",
    "                    top_k = get_top_k(probs[i], k=1, label_map=label_names)\n",
    "                    predicted_label = top_k[0][0]\n",
    "                    actual_label = label_names[labels[i].numpy()]\n",
    "                    if isinstance(actual_label, bytes):\n",
    "                        actual_label = actual_label.decode('utf8')\n",
    "                    if predicted_label == actual_label:\n",
    "                        correct_predictions += 1\n",
    "                    total_predictions += 1\n",
    "            \n",
    "            # Calculate accuracy and latency statistics\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            avg_latency = np.mean(latencies)\n",
    "            results[model_type] = {\n",
    "                'accuracy': accuracy,\n",
    "                'correct_predictions': correct_predictions,\n",
    "                'total_predictions': total_predictions,\n",
    "                'latency': avg_latency\n",
    "            }\n",
    "            print(f\"Model {model_type}: Accuracy = {accuracy:.4f}, Avg Latency = {avg_latency:.2f} ms\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation of model {model_type}: {e}\")\n",
    "            results[model_type] = {\n",
    "                'error': str(e),\n",
    "                'accuracy': None,\n",
    "                'latency': None\n",
    "            }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d1cb091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T11:08:33.875070Z",
     "iopub.status.busy": "2024-11-28T11:08:33.874767Z",
     "iopub.status.idle": "2024-11-28T15:41:20.944932Z",
     "shell.execute_reply": "2024-11-28T15:41:20.943933Z"
    },
    "papermill": {
     "duration": 16367.092553,
     "end_time": "2024-11-28T15:41:20.948335",
     "exception": false,
     "start_time": "2024-11-28T11:08:33.855782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MoViNet-A3 with resolution 256\n",
      "\n",
      "Train-Test Split Distribution plot saved for A3 with 25.0% split.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732792116.056368      23 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movinet_a3_stream/\n",
      "movinet_a3_stream/ckpt-1.data-00000-of-00001\n",
      "movinet_a3_stream/ckpt-1.index\n",
      "movinet_a3_stream/checkpoint\n",
      "\n",
      "Loaded weights for MoViNet-A3 from movinet_a3_stream/ckpt-1\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732792253.859776   11142 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732792257.519189   11143 service.cc:148] XLA service 0x7bb78e7c19a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732792257.519222   11143 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1732792257.700685   11143 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    257/Unknown - 556s 2s/step - loss: 0.9999 - accuracy: 0.7071\n",
      "Epoch 1: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 716s 2s/step - loss: 0.9999 - accuracy: 0.7071 - val_loss: 0.5384 - val_accuracy: 0.8125 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8782\n",
      "Epoch 2: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 553s 2s/step - loss: 0.4282 - accuracy: 0.8782 - val_loss: 0.4076 - val_accuracy: 0.8692 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9240\n",
      "Epoch 3: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 551s 2s/step - loss: 0.2557 - accuracy: 0.9240 - val_loss: 0.4956 - val_accuracy: 0.8750 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9440\n",
      "Epoch 4: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 553s 2s/step - loss: 0.1950 - accuracy: 0.9440 - val_loss: 0.2774 - val_accuracy: 0.9273 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9547\n",
      "Epoch 5: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 545s 2s/step - loss: 0.1503 - accuracy: 0.9547 - val_loss: 0.1561 - val_accuracy: 0.9593 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9678\n",
      "Epoch 6: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 538s 2s/step - loss: 0.1142 - accuracy: 0.9678 - val_loss: 0.5363 - val_accuracy: 0.9012 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9630\n",
      "Epoch 7: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 535s 2s/step - loss: 0.1277 - accuracy: 0.9630 - val_loss: 0.4301 - val_accuracy: 0.9041 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9581\n",
      "Epoch 8: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 538s 2s/step - loss: 0.1525 - accuracy: 0.9581 - val_loss: 0.3430 - val_accuracy: 0.8968 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9615\n",
      "Epoch 9: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 533s 2s/step - loss: 0.1414 - accuracy: 0.9615 - val_loss: 0.4681 - val_accuracy: 0.8808 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9737\n",
      "Epoch 10: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 534s 2s/step - loss: 0.1000 - accuracy: 0.9737 - val_loss: 0.0923 - val_accuracy: 0.9724 - lr: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9932\n",
      "Epoch 11: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 533s 2s/step - loss: 0.0347 - accuracy: 0.9932 - val_loss: 0.0765 - val_accuracy: 0.9811 - lr: 9.0484e-04 - learning_rate: 9.0484e-04\n",
      "Epoch 12/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9893\n",
      "Epoch 12: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 534s 2s/step - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.5063 - val_accuracy: 0.8706 - lr: 8.1873e-04 - learning_rate: 8.1873e-04\n",
      "Epoch 13/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9946\n",
      "Epoch 13: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 532s 2s/step - loss: 0.0235 - accuracy: 0.9946 - val_loss: 0.0834 - val_accuracy: 0.9797 - lr: 7.4082e-04 - learning_rate: 7.4082e-04\n",
      "Epoch 14/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9990\n",
      "Epoch 14: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 533s 2s/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 0.0904 - val_accuracy: 0.9767 - lr: 6.7032e-04 - learning_rate: 6.7032e-04\n",
      "Epoch 15/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9971\n",
      "Epoch 15: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 538s 2s/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 0.0911 - val_accuracy: 0.9811 - lr: 6.0653e-04 - learning_rate: 6.0653e-04\n",
      "Epoch 16/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9951\n",
      "Epoch 16: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 539s 2s/step - loss: 0.0227 - accuracy: 0.9951 - val_loss: 0.1607 - val_accuracy: 0.9724 - lr: 5.4881e-04 - learning_rate: 5.4881e-04\n",
      "Epoch 17/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9937\n",
      "Epoch 17: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 539s 2s/step - loss: 0.0207 - accuracy: 0.9937 - val_loss: 0.0810 - val_accuracy: 0.9855 - lr: 4.9659e-04 - learning_rate: 4.9659e-04\n",
      "Epoch 18/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 18: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 538s 2s/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0741 - val_accuracy: 0.9840 - lr: 4.4933e-04 - learning_rate: 4.4933e-04\n",
      "Epoch 19/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9985\n",
      "Epoch 19: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 538s 2s/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 0.0809 - val_accuracy: 0.9840 - lr: 4.0657e-04 - learning_rate: 4.0657e-04\n",
      "Epoch 20/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 20: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 534s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9840 - lr: 3.6788e-04 - learning_rate: 3.6788e-04\n",
      "Epoch 21/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 5.1877e-04 - accuracy: 1.0000\n",
      "Epoch 21: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 542s 2s/step - loss: 5.1877e-04 - accuracy: 1.0000 - val_loss: 0.0793 - val_accuracy: 0.9855 - lr: 3.3287e-04 - learning_rate: 3.3287e-04\n",
      "Epoch 22/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 2.3673e-04 - accuracy: 1.0000\n",
      "Epoch 22: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 544s 2s/step - loss: 2.3673e-04 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9840 - lr: 3.0119e-04 - learning_rate: 3.0119e-04\n",
      "Epoch 23/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 3.7715e-04 - accuracy: 1.0000\n",
      "Epoch 23: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 542s 2s/step - loss: 3.7715e-04 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 0.9869 - lr: 2.7253e-04 - learning_rate: 2.7253e-04\n",
      "Epoch 24/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 2.4451e-04 - accuracy: 1.0000\n",
      "Epoch 24: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 546s 2s/step - loss: 2.4451e-04 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9840 - lr: 2.4660e-04 - learning_rate: 2.4660e-04\n",
      "Epoch 25/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 1.1507e-04 - accuracy: 1.0000\n",
      "Epoch 25: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 541s 2s/step - loss: 1.1507e-04 - accuracy: 1.0000 - val_loss: 0.0794 - val_accuracy: 0.9840 - lr: 2.2313e-04 - learning_rate: 2.2313e-04\n",
      "Epoch 26/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 1.5233e-04 - accuracy: 1.0000\n",
      "Epoch 26: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 539s 2s/step - loss: 1.5233e-04 - accuracy: 1.0000 - val_loss: 0.0794 - val_accuracy: 0.9840 - lr: 2.0190e-04 - learning_rate: 2.0190e-04\n",
      "Epoch 27/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 1.3536e-04 - accuracy: 1.0000\n",
      "Epoch 27: saving model to trained_model.weights.h5\n",
      "257/257 [==============================] - 542s 2s/step - loss: 1.3536e-04 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9840 - lr: 1.8268e-04 - learning_rate: 1.8268e-04\n",
      "Epoch 28/30\n",
      "257/257 [==============================] - ETA: 0s - loss: 3.0931e-04 - accuracy: 1.0000\n",
      "Epoch 28: saving model to trained_model.weights.h5\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.264934876933694e-05.\n",
      "257/257 [==============================] - 542s 2s/step - loss: 3.0931e-04 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9840 - lr: 1.6530e-04 - learning_rate: 8.2649e-05\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "86/86 [==============================] - 128s 1s/step - loss: 0.0741 - accuracy: 0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3868072059.py:23: MatplotlibDeprecationWarning:\n",
      "\n",
      "The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 16s 16s/step\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 244ms/step\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "Enhanced confusion matrix saved as confusion_matrix_A3_test_accuracy_98.4012.png\n",
      "Classification report saved as classification_report_A3_test_at_training_split_75.0_25.0.csv\n",
      "\n",
      "Accuracy: 0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732808100.352265      23 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732808100.352319      23 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1732808103.020844      23 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "W0000 00:00:1732808461.656117      23 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732808461.656162      23 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def setup_and_train_movinet_models(models_dict):\n",
    "    for model_id, params in models_dict.items():\n",
    "        batch_size = params['batch_size']\n",
    "        num_frames = params['num_frames']\n",
    "        resolution = params['resolution']\n",
    "        split = params['split']\n",
    "\n",
    "        print(f\"Setting up MoViNet-{model_id} with resolution {resolution}\\n\")\n",
    "        \n",
    "        train_dict, test_dict = train_test_split(t, test_split=split)\n",
    "        per_model(t, train_dict, test_dict, model_id, split)\n",
    "        \n",
    "        train_generator = FrameGenerator(train_dict, num_frames, resolution, training=True)\n",
    "        test_generator = FrameGenerator(test_dict, num_frames, resolution, training=False)\n",
    "\n",
    "        train_ds = prepare_dataset(train_generator, batch_size)\n",
    "        test_ds = prepare_dataset(test_generator, batch_size)\n",
    "        \n",
    "        use_positional_encoding = model_id in {'A3', 'A4', 'A5'}\n",
    "\n",
    "        backbone = movinet.Movinet(\n",
    "            model_id=model_id.lower(),\n",
    "            causal=True,\n",
    "            conv_type='2plus1d',\n",
    "            se_type='2plus3d',\n",
    "            activation='hard_swish',\n",
    "            gating_activation='hard_sigmoid',\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "            use_external_states=False\n",
    "        )\n",
    "\n",
    "        backbone.Trainable = False\n",
    "\n",
    "        model = movinet_model.MovinetClassifier(\n",
    "            backbone,\n",
    "            num_classes=600,\n",
    "            output_states=True\n",
    "        )\n",
    "\n",
    "        # Load pre-trained weights\n",
    "        url = f\"https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_{model_id.lower()}_stream.tar.gz\"\n",
    "        tar_filename = f\"movinet_{model_id.lower()}_stream.tar.gz\"\n",
    "        checkpoint_dir = f\"movinet_{model_id.lower()}_stream\"\n",
    "\n",
    "        os.system(f\"wget {url} -O {tar_filename} -q\")\n",
    "        os.system(f\"tar -xvf {tar_filename}\")\n",
    "\n",
    "        checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        checkpoint = tf.train.Checkpoint(model=model)\n",
    "        status = checkpoint.restore(checkpoint_path)\n",
    "        status.assert_existing_objects_matched()\n",
    "        print(f\"\\nLoaded weights for MoViNet-{model_id} from {checkpoint_path}\")\n",
    "\n",
    "        model = build_classifier(batch_size, num_frames, resolution, backbone, 8)\n",
    "        \n",
    "        loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(loss=loss_obj, optimizer='adam', metrics=['accuracy'])\n",
    "        lr_scheduler = CustomLearningRateScheduler(lr_schedule)\n",
    "\n",
    "        callbacks = [\n",
    "            CustomLearningRateLogger(),\n",
    "            CustomModelCheckpoint(\n",
    "                filepath=\"trained_model.weights.h5\",\n",
    "                save_weights_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            CustomReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=10,\n",
    "                min_lr=1e-10,\n",
    "                verbose=1,\n",
    "                cooldown=1,\n",
    "                min_delta=1e-3\n",
    "            ),\n",
    "            CustomEarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            lr_scheduler,\n",
    "            CustomTensorBoard(log_dir='./logs')\n",
    "        ]\n",
    "\n",
    "        results = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=test_ds,\n",
    "            epochs=30,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluation_results = model.evaluate(test_ds,return_dict=True)\n",
    "        accuracy = evaluation_results['accuracy']\n",
    "    \n",
    "\n",
    "        # Plot training metrics\n",
    "        plot_training_metrics(results, model_id, 0.001, batch_size, 'adam', resolution, split, accuracy)\n",
    "       \n",
    "        # Get actual and predicted labels\n",
    "        actual, predicted = get_actual_predicted_labels(test_ds, model)\n",
    "        label_names = list(test_generator.class_ids_for_name.keys())\n",
    "                \n",
    "        plot_confusion_matrix(actual, predicted, label_names, 'test', model_id,accuracy,split)\n",
    "        plot_confusion_matrix2(actual, predicted, label_names, 'test', model_id,accuracy,split)\n",
    "    \n",
    "        print(f'\\nAccuracy: {accuracy:.4f}')     \n",
    "        \n",
    "        model_id = model_id.lower()\n",
    "        use_positional_encoding = model_id in {'a3', 'a4', 'a5'}\n",
    "        resolution = resolution\n",
    "\n",
    "        # Create backbone and model.\n",
    "        backbone = movinet.Movinet(\n",
    "            model_id=model_id,\n",
    "            causal=True,\n",
    "            conv_type='2plus1d',\n",
    "            se_type='2plus3d',\n",
    "            activation='hard_swish',\n",
    "            gating_activation='hard_sigmoid',\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "            use_external_states=True,\n",
    "        )\n",
    "\n",
    "        model = movinet_model.MovinetClassifier(\n",
    "                backbone,\n",
    "                num_classes=8,\n",
    "                output_states=True)\n",
    "\n",
    "        model.load_weights('/kaggle/working/trained_model.weights.h5')\n",
    "        # Export model variants\n",
    "        exported_models = export_model_variants(model, model_id, resolution,accuracy, split, train_dict, '/kaggle/working/trained_model.weights.h5')\n",
    "        # Evaluate exported TFLite models\n",
    "#         tflite_results = evaluate_exported_models(exported_models, test_ds, label_names, model_id, resolution, split)\n",
    "#         plot_tflite_evaluation_results(tflite_results, label_names, model_id, resolution, split)\n",
    "        \n",
    "setup_and_train_movinet_models(models)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5535876,
     "sourceId": 9162766,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16597.834766,
   "end_time": "2024-11-28T15:41:26.904827",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-28T11:04:49.070061",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
